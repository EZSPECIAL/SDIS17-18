---------------------
ORDER
---------------------

    - swap to linux
    - delete
    - restore
    - reclaim
    - enh backup
    - enh restore
    - enh delete
    - protocol version dependent behavior (1.0 base 1.1 enh)

concurrentlinkedqueue -> channels send here

---------------------
GRADE
---------------------

    - 70% (concurrent with all protocols)
    - 15% - 5% per enhancement
    - 5% - RMI
    - 5% - concurrency model in report
    - 5% - demo setup

---------------------
THREADING
---------------------

Version 2

    - one protocol instance at a time
    - one thread per channel (control, backup, restore)
    - msg thread is endless loop of receive -> process
    - multicaster thread (thread.sleep() for timeout), created for TestApp, terminated on protocol finish
    - TestApp RMI creates multicaster thread to handle protocol invoked
    - multicaster and control channel thread need to share received STORED, use locks
    - non-initiator peers don't need to handle STORED but could since the code is already there

Version 3

    - multiple multicaster instances on any Peer, handle 1 protocol (send N messages with sliding window)
    - one thread per channel, keeping protocol instances state in objects
    - initiator creates object on transmit, receiver on receive
    - java.util.concurrent.ConcurrentHashMap to store protocol state

Version 4

    - receiver threads now use worker threads for processing so that they can keep listening
    - java.util.concurrent.ThreadPoolExecutor

Version 5

    - no thread.sleep() with java.util.concurrent.ScheduledThreadPoolExecutor which allows a "timeout" handler

Version 6

    - I/O concurrency with java.nio.channels.AsynchronousFileChannel

---------------------
GENERAL
---------------------

Peers

    - have a max disk space and keep count of chunk disk space usage (start with 8MB)

Generic message

    - header / body (data)
    - 1+ space between fields
    - 0+ spaces on last field
    - header termination <CRLF><CRLF>
    - new/modified messages can have more header fields, ignore them in parser
    - drop any unrecognized message

    - message type (protocol)
    - version ("1.0")
    - senderID (peer id)
    - fileID (sha256 with last modified metadata, encoded as HEX to ASCII)
    - chunkNo (start at 0, up to 6 digits)
    - repDegree (1 to 9, store N copies in system)

    - body (file divided in 64K chunks identified by (fileID, chunkNo), last chunk has 0 to 63999 size)

---------------------
PROTOCOLS
---------------------

Restore

    Initiator
    - MC -> GETCHUNK <Version> <SenderId> <FileId> <ChunkNo> <CRLF><CRLF>
    - save file with _restore appended "file_restored.png" or "file_restored" if no extension
    - save in same filepath as original file or a "restored" folder
    
    Peers (that have the file-chunk)
    - MDR -> CHUNK <Version> <SenderId> <FileId> <ChunkNo> <CRLF><CRLF><Body>
    - random delay between 0 and 400ms
    - "To avoid flooding the host with CHUNK messages, each peer shall wait for a random time uniformly distributed
between 0 and 400 ms, before sending the CHUNK message. If it receives a CHUNK message before that time expires,
it will not send the CHUNK message."

Delete

    Initiator
    - MC -> DELETE <Version> <SenderId> <FileId> <CRLF><CRLF>
    - all peers delete their chunks of the file
    - send N times to ensure reception of message

Reclaim

    Initiator
    - MC -> REMOVED <Version> <SenderId> <FileId> <ChunkNo> <CRLF><CRLF>
    - Peers receiving this message that have the chunk update the local repDeg count
    - If repDeg count is below desired for that chunk, starts Backup protocol with delay (0 to 400ms)
    - If during 0 to 400ms delay a PUTCHUNK for the same chunk is received, abort

---------------------
REPORT
---------------------

    - 1 page per enhancement (backup, restore, delete)
    - 2-3 pages for concurrency model

---------------------
SETUP
---------------------

    - scripts for quick testing
    - test PCs in lab, local and remote

---------------------
CLIENT
---------------------

    "A peer should also count the number of confirmation messages for each of the chunks it has stored and keep that
    count in non-volatile memory. This information can be useful if the peer runs out of disk space: in that event,
    the peer may try to free some space by evicting chunks whose actual replication degree is higher than the desired
    replication degree."

    Get info ->
        For each file Peer has initiated backup of
            - file pathname
            - backup file service ID
            - desired repDeg
        For each chunk of the file
            - id
            - perceived repDeg
        For each chunk it stores
            - id
            - size in KB
            - perceived repDeg
        Curr KB used / max KB possible

---------------------
ENHANCEMENTS
---------------------

Backup

    - ensure repDegre
    - keep interoperability
    - what if not enough peers or disk space on them?

Restore (using TCP)

    - ServerSocket - start TCP connection
    - Socket - transfer data
    - ServerSocket(int port)
    - ServerSocket(int port, int backlog, InetAddress addr)
    - Socket(InetAddress addr, int port)

    - socket.accept() - wait for connection
    - socket.close() / shutdownOutput()

    - getInputStream() BufferedReader -> ReadLine()
    - getOutputStream() PrintWriter -> println()
    - ObjectInputStream() implements Serializable
    - ObjectOutputStream() implements Serializable
    
    - multiple clients

Delete

    - delete chunks even if Peer is currently offline

If adding messages

    - add a new header line
    - change protocol version to 1.1+

---------------------
EXTRAS
---------------------

    - every protocol playing nice with each other
    - monitor file system changes, eg deletion (unconfirmed)
    - RMI over network, "//host/name" format (confirmed)
    - RESTORE: concurrent same chunk requests from different Peers
    - RESTORE: worry about not getting the chunks

---------------------
QUESTIONS
---------------------

RMI: sometimes Enter key is needed for output to appear in Peer/Server

Filesystem delete - affects service?

Offline Peer - doesn't this mean it loses all info? How can the delete enhancement work? How can "get info" work if the Peer loses info of what it initiated? What about syncing filelist between Peers?

DELETE Enhancement general idea / interop
(if deletion triggers "REMOVED" message, every Peer could keep where each chunk is located (Peer ID) and know that deletion
 wasn't complete until every expected "REMOVED" msg arrived. However offline Peers would have deprecated file list, don't see a way without new messages/protocol for keeping filelist updated)